import streamlit as st
import pandas as pd
import ast
import requests
from bs4 import BeautifulSoup
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# 1. æ•°æ®åŠ è½½
@st.cache_data
def load_data():
    df = pd.read_csv("../Anime_data.csv")
    df['Genre'] = df['Genre'].apply(lambda x: ' '.join(ast.literal_eval(x)) if pd.notnull(x) else '')
    return df

df = load_data()

# 2. è·å–åŠ¨æ¼«å°é¢å›¾
@st.cache_data
def get_cover_image(url):
    try:
        response = requests.get(url, headers={"User-Agent": "Mozilla/5.0"})
        soup = BeautifulSoup(response.text, "html.parser")
        img_tag = soup.find("img", {"itemprop": "image"})
        if img_tag:
            return img_tag.get("data-src") or img_tag.get("src")
    except Exception:
        return None
    return None

# 3. ç›¸ä¼¼åº¦è®¡ç®—å‡½æ•°ï¼ˆä¼ å…¥è¿‡æ»¤åçš„ dfï¼‰
@st.cache_data(show_spinner=False)
def compute_similarity(filtered_df):
    tfidf = TfidfVectorizer(stop_words='english')
    tfidf_matrix = tfidf.fit_transform(filtered_df['Genre'])
    cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)
    return cosine_sim

# 4. æ¨èå‡½æ•°ï¼ˆæŒ‰è¯„åˆ†æ’åº + æ­£ç¡®è¿‡æ»¤ï¼‰
def recommend(anime_title, top_n=6, filter_hentai=True):
    if filter_hentai:
        df_filtered = df[~df['Genre'].str.contains("Hentai", case=False, na=False)].reset_index(drop=True)
    else:
        df_filtered = df.reset_index(drop=True)

    if anime_title not in df_filtered['Title'].values:
        st.warning(f"æœªæ‰¾åˆ°åä¸º '{anime_title}' çš„åŠ¨æ¼«ï¼Œå¯èƒ½æ˜¯å› ä¸ºè¯¥ç±»å‹è¢«è¿‡æ»¤æ‰äº†ã€‚")
        if anime_title in df['Title'].values:
            st.warning("æ³¨æ„ï¼šè¯¥åŠ¨æ¼«åŸæœ¬å­˜åœ¨ï¼Œä½†å®ƒåŒ…å«äº† 'Hentai' ç±»å‹ï¼Œå› æ­¤è¢«è¿‡æ»¤æ‰ã€‚")
        return pd.DataFrame()

    cosine_sim_filtered = compute_similarity(df_filtered)
    idx = df_filtered[df_filtered['Title'] == anime_title].index[0]
    sim_scores = list(enumerate(cosine_sim_filtered[idx]))
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
    sim_scores = sim_scores[1:top_n + 1]
    anime_indices = [i[0] for i in sim_scores]
    return df_filtered.iloc[anime_indices][['Title', 'Genre', 'Rating', 'Link']].sort_values(by='Rating', ascending=False)

# 5. å…³é”®è¯æ¨èå‡½æ•°ï¼ˆä¸ä½¿ç”¨ç›¸ä¼¼åº¦ï¼‰
def recommend_by_keyword(keyword, top_n=6, filter_hentai=True):
    if filter_hentai:
        filtered_df = df[~df['Genre'].str.contains("Hentai", case=False, na=False)]
    else:
        filtered_df = df
    
    filtered_df = filtered_df[filtered_df['Genre'].str.contains(keyword, case=False, na=False)]
    return filtered_df[['Title', 'Genre', 'Rating', 'Link']].sort_values(by='Rating', ascending=False).head(top_n)

# 6. Streamlit é¡µé¢è®¾ç½®
st.set_page_config(page_title="Anime Recommender", layout="wide")

# åˆå§‹åŒ– session_state
if 'recommended_count' not in st.session_state:
    st.session_state.recommended_count = 6
if 'filter_hentai' not in st.session_state:
    st.session_state.filter_hentai = True

# é¡µé¢å¯¼èˆª
st.sidebar.title("é€‰æ‹©é¡µé¢")
page = st.sidebar.radio("é¡µé¢", ("é¦–é¡µ", "å…³é”®è¯æ¨è", "è®¾ç½®"))

# è®¾ç½®é¡µé¢
if page == "è®¾ç½®":
    st.title("âš™ï¸ è®¾ç½®")
    st.write("è‡ªå®šä¹‰æ¨èæ•°é‡å’Œæ˜¯å¦è¿‡æ»¤ Hentai ç±»å‹")

    recommended_count = st.slider("æ¯æ¬¡æ¨èæ•°é‡", min_value=3, max_value=21, value=st.session_state.recommended_count, step=3)
    st.session_state.recommended_count = recommended_count

    filter_hentai = st.checkbox("è¿‡æ»¤ Hentai ç±»å‹åŠ¨æ¼«", value=st.session_state.filter_hentai)
    st.session_state.filter_hentai = filter_hentai

# é¦–é¡µæ¨è
elif page == "é¦–é¡µ":
    st.title("ğŸ¯ Anime Recommender System")
    st.write("é€‰æ‹©ä½ å–œæ¬¢çš„åŠ¨æ¼«ï¼Œç³»ç»Ÿä¼šæ¨èç›¸ä¼¼çš„ä½œå“ï¼ˆæŒ‰è¯„åˆ†æ’åºï¼‰")

    anime_list = df['Title'].dropna().unique()
    selected_anime = st.selectbox("é€‰æ‹©åŠ¨æ¼«", anime_list)

    if st.button("æ¨è"):
        results = recommend(
            selected_anime,
            top_n=st.session_state.get('recommended_count', 6),
            filter_hentai=st.session_state.get('filter_hentai', True)
        )

        if results.empty:
            st.warning("æœªæ‰¾åˆ°ç›¸å…³åŠ¨æ¼«")
        else:
            for row_start in range(0, len(results), 3):
                cols = st.columns(3, gap="large")
                for col, (_, row) in zip(cols, results.iloc[row_start:row_start + 3].iterrows()):
                    with col:
                        img_url = get_cover_image(row['Link'])
                        if img_url:
                            st.image(img_url, width=150)
                        st.markdown(f"**[{row['Title']}]({row['Link']})**")
                        st.write(f"â­ {row['Rating']}")
                        st.caption(row['Genre'])
                st.markdown("<br><br>", unsafe_allow_html=True)

# å…³é”®è¯æ¨è
elif page == "å…³é”®è¯æ¨è":
    st.title("ğŸ” åŸºäºå…³é”®è¯çš„åŠ¨æ¼«æ¨è")
    st.write("è¾“å…¥ä½ å–œæ¬¢çš„ç±»å‹æˆ–ä¸»é¢˜ï¼Œæˆ‘ä»¬å°†æ¨èç›¸å…³åŠ¨æ¼«")

    keyword = st.text_input("è¾“å…¥å…³é”®è¯", "æˆ˜æ–—")

    if st.button("æ¨è"):
        results = recommend_by_keyword(
            keyword,
            top_n=st.session_state.get('recommended_count', 6),
            filter_hentai=st.session_state.get('filter_hentai', True)
        )

        if results.empty:
            st.warning("æ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„åŠ¨æ¼«")
        else:
            for row_start in range(0, len(results), 3):
                cols = st.columns(3, gap="large")
                for col, (_, row) in zip(cols, results.iloc[row_start:row_start + 3].iterrows()):
                    with col:
                        img_url = get_cover_image(row['Link'])
                        if img_url:
                            st.image(img_url, width=150)
                        st.markdown(f"**[{row['Title']}]({row['Link']})**")
                        st.write(f"â­ {row['Rating']}")
                        st.caption(row['Genre'])
                st.markdown("<br><br>", unsafe_allow_html=True)
